{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07cd0579",
   "metadata": {},
   "source": [
    "# PDF Batch Ingestion + JSONL Export (Thai/English)\n",
    "\n",
    "ประมวลผลเอกสาร PDF ทั้งโฟลเดอร์: ดึงข้อความ (PyMuPDF) + OCR fallback (pdf2image + Tesseract) + ทำความสะอาด และส่งออกเป็น JSONL ต่อหน้า พร้อม .txt ต่อไฟล์\n",
    "\n",
    "- ใช้ร่วมกับสมุดโน้ต: `pdf_ingest_normalize.ipynb` (โหมดเดี่ยวไฟล์)\n",
    "- โน้ตบุ๊กนี้โฟกัสแบบ batch + JSONL/ข้อความต่อไฟล์\n",
    "\n",
    "Dependencies: pymupdf, pdf2image, pytesseract, pillow, unidecode (ตัวเลือก: pythainlp)\n",
    "\n",
    "Windows Notes:\n",
    "- ติดตั้ง Tesseract-OCR และกำหนด path ในโค้ดถ้าไม่อยู่ใน PATH\n",
    "- ติดตั้ง Poppler for Windows และตั้งค่าโฟลเดอร์ `.../Library/bin` ในตัวแปร `POPPLER_BIN` หรือในโค้ด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ติดตั้งแพ็กเกจที่จำเป็น (รันครั้งแรก)\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', *pkgs])\n",
    "    except Exception as e:\n",
    "        print('Package install finished with message:', e)\n",
    "\n",
    "pkgs = [\n",
    "    'pymupdf',\n",
    "    'pdf2image',\n",
    "    'pytesseract',\n",
    "    'pillow',\n",
    "    'unidecode',\n",
    "    'pythainlp',  # optional\n",
    "    'requests',   # AksonOCR HTTP client\n",
    "    'pandas',     # Excel/CSV ingestion\n",
    "    'openpyxl',   # Excel engine (.xlsx)\n",
    "    'xlrd',       # Legacy Excel (.xls)\n",
    "]\n",
    "#pip_install(pkgs)  # ยกเลิกคอมเมนต์ถ้าต้องการติดตั้งจากโน้ตบุ๊ก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd1d546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesseract : C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\n",
      "Poppler bin: C:\\\\Program Files\\\\poppler-25.07.0\\\\Library\\\\bin\n",
      "AksonOCR  : disabled\n"
     ]
    }
   ],
   "source": [
    "# Imports และการตั้งค่า Tesseract / Poppler (Windows)\n",
    "import os, re, unicodedata, shutil, mimetypes, time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "import fitz  # PyMuPDF\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import requests\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "def ensure_tesseract_cmd() -> Optional[str]:\n",
    "    cmd = getattr(pytesseract.pytesseract, 'tesseract_cmd', None)\n",
    "    if cmd and os.path.exists(cmd):\n",
    "        return cmd\n",
    "    if os.name == 'nt':\n",
    "        for c in [\n",
    "            r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe',\n",
    "            r'C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe',\n",
    "        ]:\n",
    "            if os.path.exists(c):\n",
    "                pytesseract.pytesseract.tesseract_cmd = c\n",
    "                return c\n",
    "        print('WARNING: ไม่พบ Tesseract-OCR บน Windows. ดู https://github.com/UB-Mannheim/tesseract/wiki')\n",
    "        return None\n",
    "    return shutil.which('tesseract')\n",
    "\n",
    "def guess_poppler_bin() -> Optional[str]:\n",
    "    if os.name != 'nt':\n",
    "        return None\n",
    "    for c in [\n",
    "            r'C:\\\\Program Files\\\\poppler-24.08.0\\\\Library\\\\bin',\n",
    "            r'C:\\\\Program Files\\\\poppler-25.07.0\\\\Library\\\\bin',\n",
    "            r'C:\\\\Program Files\\\\poppler-0.68.0\\\\bin',\n",
    "        ]:\n",
    "        if os.path.exists(c):\n",
    "            return c\n",
    "    env = os.getenv('POPPLER_BIN') or os.getenv('POPPLER_PATH')\n",
    "    if env and os.path.exists(env):\n",
    "        return env\n",
    "    return None\n",
    "\n",
    "POPPLER_BIN = guess_poppler_bin()\n",
    "TESS_CMD = ensure_tesseract_cmd()\n",
    "# ใช้ตัวแปรแวดล้อมมาตรฐาน 'AKSONOCR_API_KEY'\n",
    "AKSONOCR_API_KEY = os.environ.get('AKSONOCR_API_KEY')  # ใช้ถ้าต้องการ OCR ผ่าน AksonOCR\n",
    "print('Tesseract :', TESS_CMD or 'not found')\n",
    "print('Poppler bin:', POPPLER_BIN or 'not set (Windows requires this for pdf2image)')\n",
    "print('AksonOCR  :', 'enabled' if AKSONOCR_API_KEY else 'disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d6dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ฟังก์ชันหลัก: extract (MuPDF/OCR), normalize, Thai tidy, paragraphing, header/footer\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as _pd\n",
    "\n",
    "def extract_text_mupdf(pdf_path: str) -> str:\n",
    "    texts: List[str] = []\n",
    "    with fitz.open(str(pdf_path)) as doc:\n",
    "        for page in doc:\n",
    "            try:\n",
    "                txt = page.get_text('text')\n",
    "            except Exception:\n",
    "                txt = page.get_text()\n",
    "            texts.append(txt or '')\n",
    "    return '\\n'.join(texts)\n",
    "\n",
    "def ocr_pdf(pdf_path: str, dpi: int = 300, lang: str = 'tha+eng', poppler_bin: Optional[str] = None) -> str:\n",
    "    kwargs = {}\n",
    "    if (poppler_bin or POPPLER_BIN) and os.name == 'nt':\n",
    "        kwargs['poppler_path'] = poppler_bin or POPPLER_BIN\n",
    "    images = convert_from_path(pdf_path, dpi=dpi, **kwargs)\n",
    "    texts = [pytesseract.image_to_string(img, lang=lang) for img in images]\n",
    "    return '\\n'.join(texts)\n",
    "\n",
    "def ocr_request_with_retry(image_path: str, api_key: str, max_retries: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    ส่ง OCR ไปที่ AksonOCR พร้อม retry เมื่อโดน rate limit (HTTP 429)\n",
    "    \"\"\"\n",
    "    url = 'https://backend.aksonocr.com/api/v1/ocr'\n",
    "    headers = {'X-API-Key': api_key}\n",
    "    mime_type = mimetypes.guess_type(image_path)[0] or 'image/png'\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with open(image_path, 'rb') as f:\n",
    "                files = {'file': (image_path, f, mime_type)}\n",
    "                data = {'model': 'AksonOCR-preview'}\n",
    "                resp = requests.post(url, headers=headers, files=files, data=data, timeout=60)\n",
    "            if 'X-RateLimit-Remaining' in resp.headers:\n",
    "                print(f\"Rate limit remaining: {resp.headers['X-RateLimit-Remaining']}\")\n",
    "            if resp.status_code == 429:\n",
    "                retry_after = int(resp.headers.get('Retry-After', 60))\n",
    "                print(f'Rate limit exceeded. Retrying after {retry_after} seconds...')\n",
    "                time.sleep(retry_after)\n",
    "                continue\n",
    "            result = resp.json()\n",
    "            if result.get('success', True):\n",
    "                return {'success': True, 'text': (result.get('data') or {}).get('text', ''), 'data': result.get('data')}\n",
    "            return {'success': False, 'error': result.get('error') or {'message': 'Unknown error'}}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "                continue\n",
    "            return {'success': False, 'error': {'message': str(e)}}\n",
    "    return {'success': False, 'error': {'message': 'Max retries exceeded'}}\n",
    "\n",
    "def akson_ocr_image_pil(img: Image.Image, api_key: str) -> str:\n",
    "    \"\"\"OCR ภาพหนึ่งหน้าผ่าน AksonOCR โดยบันทึกเป็นไฟล์ชั่วคราว\"\"\"\n",
    "    with NamedTemporaryFile(suffix='.png', delete=True) as tmp:\n",
    "        img.save(tmp.name, format='PNG')\n",
    "        res = ocr_request_with_retry(tmp.name, api_key)\n",
    "        return res.get('text', '') if res.get('success') else ''\n",
    "\n",
    "def ocr_pdf_akson(pdf_path: str, dpi: int = 450, api_key: str | None = None, poppler_bin: Optional[str] = None) -> str:\n",
    "    \"\"\"OCR ทั้งไฟล์ผ่าน AksonOCR (render ด้วย pdf2image)\"\"\"\n",
    "    if not api_key:\n",
    "        return ''\n",
    "    kwargs = {}\n",
    "    if (poppler_bin or POPPLER_BIN) and os.name == 'nt':\n",
    "        kwargs['poppler_path'] = poppler_bin or POPPLER_BIN\n",
    "    images = convert_from_path(pdf_path, dpi=dpi, **kwargs)\n",
    "    texts = [akson_ocr_image_pil(img, api_key) for img in images]\n",
    "    return '\\n'.join(texts)\n",
    "\n",
    "def normalize_text(text: str, preserve_newlines: bool = True) -> str:\n",
    "    if text is None:\n",
    "        return ''\n",
    "    t = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    t = unicodedata.normalize('NFC', t)\n",
    "    t = t.replace('\\u00A0', ' ')\n",
    "    t = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', t)\n",
    "    t = re.sub(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F]', '', t)\n",
    "    if preserve_newlines:\n",
    "        lines = [re.sub(r'[ \\t]+', ' ', ln).strip() for ln in t.split('\\n')]\n",
    "        t = '\\n'.join(lines)\n",
    "        t = re.sub(r'\\n{3,}', '\\n\\n', t)\n",
    "    else:\n",
    "        t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    return t\n",
    "\n",
    "def text_quality_score(text: str) -> float:\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    chars = [c for c in text if not c.isspace()]\n",
    "    if not chars:\n",
    "        return 0.0\n",
    "    signal = sum(1 for c in chars if c.isalpha() or c.isdigit())\n",
    "    return signal / max(1, len(chars))\n",
    "\n",
    "# Thai tidy + optional PyThaiNLP normalize\n",
    "try:\n",
    "    from pythainlp.util import normalize as th_normalize\n",
    "    _HAS_THAI = True\n",
    "except Exception:\n",
    "    _HAS_THAI = False\n",
    "    def th_normalize(x: str) -> str: return x\n",
    "_TH_CHR = r'\\u0E00-\\u0E7F'\n",
    "_TH_PAIR = re.compile(rf'([{_TH_CHR}])\\s+([{_TH_CHR}])')\n",
    "\n",
    "def tidy_thai_spacing(text: str) -> str:\n",
    "    if not text: return text\n",
    "    t = _TH_PAIR.sub(r'\\1\\2', text)\n",
    "    return re.sub(r'[ \\t]+', ' ', t)\n",
    "\n",
    "def thai_postprocess(text: str) -> str:\n",
    "    t = tidy_thai_spacing(text)\n",
    "    if _HAS_THAI:\n",
    "        try: t = th_normalize(t)\n",
    "        except Exception: pass\n",
    "    return t\n",
    "\n",
    "# ยูทิลตรวจสัดส่วนสคริปต์ไทย/ละติน เพื่อเลือกภาษา OCR\n",
    "_TH_RANGE = re.compile(r'[\\u0E00-\\u0E7F]')\n",
    "_LATIN_RANGE = re.compile(r'[A-Za-z]')\n",
    "\n",
    "def script_ratios(text: str) -> tuple[float, float]:\n",
    "    if not text:\n",
    "        return 0.0, 0.0\n",
    "    th = len(_TH_RANGE.findall(text))\n",
    "    la = len(_LATIN_RANGE.findall(text))\n",
    "    total = th + la\n",
    "    if total == 0:\n",
    "        return 0.0, 0.0\n",
    "    return th / total, la / total\n",
    "\n",
    "def choose_ocr_lang_for_text(text: str, default: str = 'tha', latin_threshold: float = 0.15) -> str:\n",
    "    th_r, la_r = script_ratios(text)\n",
    "    if la_r >= latin_threshold:\n",
    "        return 'tha+eng'\n",
    "    return default\n",
    "\n",
    "# แบ่งย่อหน้าอย่างชาญฉลาดจากเนื้อหาหน้า (เพิ่มจำนวนย่อหน้าให้ละเอียดขึ้น)\n",
    "_BULLET_START = re.compile(r\"^([\\-\\•\\–\\*]|\\d+[\\.)]|[ก-ฮ]\\)|\\([0-9]+\\)|\\([ก-ฮ]\\))\\s+\")\n",
    "_SENT_SPLIT = re.compile(r\"(?<=[\\.!?…\\u0E2F\\u0E5B\\u0E46])\\s+\")\n",
    "\n",
    "def split_paragraphs_smart(text: str) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    t = text.strip()\n",
    "    # 1) break by blank lines first\n",
    "    blocks = [b.strip() for b in re.split(r\"\\n\\s*\\n+\", t) if b.strip()]\n",
    "    out: List[str] = []\n",
    "    for b in blocks:\n",
    "        lines = [ln.rstrip() for ln in b.split('\\n')]\n",
    "        buf: List[str] = []\n",
    "        for ln in lines:\n",
    "            if _BULLET_START.search(ln):\n",
    "                if buf:\n",
    "                    out.append('\\n'.join(buf).strip())\n",
    "                    buf = []\n",
    "                out.append(ln.strip())\n",
    "            else:\n",
    "                buf.append(ln)\n",
    "        if buf:\n",
    "            para = '\\n'.join(buf).strip()\n",
    "            # 2) if too long, split by sentences into ~400-600 chars groups\n",
    "            if len(para) > 1200:\n",
    "                sents = [s.strip() for s in _SENT_SPLIT.split(para) if s.strip()]\n",
    "                pack: List[str] = []\n",
    "                cur = ''\n",
    "                for s in sents:\n",
    "                    if len(cur) + 1 + len(s) > 600:\n",
    "                        if cur:\n",
    "                            pack.append(cur.strip())\n",
    "                        cur = s\n",
    "                    else:\n",
    "                        cur = (cur + ' ' + s).strip()\n",
    "                if cur:\n",
    "                    pack.append(cur.strip())\n",
    "                out.extend(pack)\n",
    "            else:\n",
    "                out.append(para)\n",
    "    # safety: drop tiny noise\n",
    "    out = [p for p in out if len(p.strip()) >= 2]\n",
    "    return out\n",
    "\n",
    "def _edge_nonempty_lines(text: str, take_top=2, take_bottom=2) -> tuple:\n",
    "    lines = [ln.strip() for ln in text.split('\\n')]\n",
    "    top, bottom = [], []\n",
    "    for ln in lines:\n",
    "        if ln: top.append(ln)\n",
    "        if len(top) >= take_top: break\n",
    "    for ln in reversed(lines):\n",
    "        if ln: bottom.append(ln)\n",
    "        if len(bottom) >= take_bottom: break\n",
    "    return top, list(reversed(bottom))\n",
    "\n",
    "def _detect_headers_footers(page_texts: list, top_k=2, bottom_k=2, min_occ: int = 3) -> tuple[set, set]:\n",
    "    top_counter, bot_counter = Counter(), Counter()\n",
    "    for txt in page_texts:\n",
    "        t, b = _edge_nonempty_lines(txt, top_k, bottom_k)\n",
    "        t = [re.sub(r'\\s+', ' ', x) for x in t]\n",
    "        b = [re.sub(r'\\s+', ' ', x) for x in b]\n",
    "        top_counter.update(t); bot_counter.update(b)\n",
    "    headers = {s for s, c in top_counter.items() if c >= min_occ and len(s) >= 5}\n",
    "    footers = {s for s, c in bot_counter.items() if c >= min_occ and len(s) >= 3}\n",
    "    return headers, footers\n",
    "\n",
    "def _strip_headers_footers(text: str, headers: set, footers: set, window: int = 4) -> tuple[str, str | None, str | None]:\n",
    "    lines = [ln.rstrip() for ln in text.split('\\n')]\n",
    "    header, footer = None, None\n",
    "    for i in range(min(window, len(lines))):\n",
    "        cand = re.sub(r'\\s+', ' ', lines[i].strip())\n",
    "        if cand in headers:\n",
    "            header = lines[i]; lines[i] = ''; break\n",
    "    for i in range(len(lines)-1, max(-1, len(lines)-1-window), -1):\n",
    "        cand = re.sub(r'\\s+', ' ', lines[i].strip())\n",
    "        if cand in footers:\n",
    "            footer = lines[i]; lines[i] = ''; break\n",
    "    cleaned = '\\n'.join(lines)\n",
    "    cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
    "    return cleaned, header, footer\n",
    "\n",
    "def extract_pages_with_fallback_to_jsonl(\n",
    "    pdf_path: str, out_jsonl_path: str, dpi: int = 300, ocr_lang: str = 'tha+eng',\n",
    "    ocr_dpi: int = 450, min_length: int = 50, min_score: float = 0.2, poppler_bin: str | None = None,\n",
    "    use_dynamic_lang: bool = True,\n",
    ") -> str:\n",
    "    doc = fitz.open(str(pdf_path))\n",
    "    n_pages = doc.page_count\n",
    "    mupdf_texts = []\n",
    "    for p in range(n_pages):\n",
    "        try: txt = doc.load_page(p).get_text('text') or ''\n",
    "        except Exception: txt = doc.load_page(p).get_text() or ''\n",
    "        mupdf_texts.append(txt)\n",
    "    doc.close()\n",
    "\n",
    "    # ประเมินระดับเอกสารสำหรับภาษาสำรอง (กรณีหน้านั้นว่าง/คุณภาพต่ำ)\n",
    "    doc_preview = '\\n'.join(mupdf_texts[:min(3, len(mupdf_texts))])\n",
    "    doc_default_lang = choose_ocr_lang_for_text(doc_preview, default='tha', latin_threshold=0.15) if use_dynamic_lang else ocr_lang\n",
    "\n",
    "    decisions, need_ocr_indices = [], []\n",
    "    for i, txt in enumerate(mupdf_texts):\n",
    "        score = text_quality_score(txt)\n",
    "        decide_ocr = (not txt.strip()) or (len(txt.strip()) < min_length) or (score < min_score)\n",
    "        decisions.append(('ocr' if decide_ocr else 'mupdf', score, len(txt.strip())))\n",
    "        if decide_ocr: need_ocr_indices.append(i)\n",
    "    ocr_texts = {}\n",
    "    if need_ocr_indices:\n",
    "        kwargs = {}\n",
    "        if (poppler_bin or POPPLER_BIN) and os.name == 'nt':\n",
    "            kwargs['poppler_path'] = poppler_bin or POPPLER_BIN\n",
    "        for i in need_ocr_indices:\n",
    "            imgs = convert_from_path(pdf_path, dpi=ocr_dpi, first_page=i+1, last_page=i+1, **kwargs)\n",
    "            if imgs:\n",
    "                if 'AKSONOCR_API_KEY' in globals() and AKSONOCR_API_KEY:\n",
    "                    # AksonOCR โดยทั่วไป auto-detect ภาษา ไม่ต้องส่งพารามิเตอร์\n",
    "                    ocr_texts[i] = akson_ocr_image_pil(imgs[0], AKSONOCR_API_KEY) or ''\n",
    "                else:\n",
    "                    # เลือกภาษา OCR ต่อหน้าแบบไดนามิก (เริ่มจากไทยก่อน หากเจออักษรละตินมากพอ → tha+eng)\n",
    "                    lang_page = ocr_lang\n",
    "                    if use_dynamic_lang:\n",
    "                        lang_page = choose_ocr_lang_for_text(mupdf_texts[i] or '', default=doc_default_lang, latin_threshold=0.15)\n",
    "                        if lang_page != ocr_lang:\n",
    "                            print(f\"[page {i+1}] OCR lang override: {ocr_lang} -> {lang_page}\")\n",
    "                    ocr_texts[i] = pytesseract.image_to_string(imgs[0], lang=lang_page) or ''\n",
    "            else:\n",
    "                ocr_texts[i] = ''\n",
    "    chosen_texts, methods = [], []\n",
    "    for i in range(n_pages):\n",
    "        method = decisions[i][0]; methods.append(method)\n",
    "        raw = ocr_texts.get(i, mupdf_texts[i]) if method == 'ocr' else mupdf_texts[i]\n",
    "        norm = clean_for_index(raw)\n",
    "        chosen_texts.append(norm)\n",
    "    headers, footers = _detect_headers_footers(chosen_texts, 2, 2, min_occ=max(3, n_pages // 4 or 1))\n",
    "    stripped, page_headers, page_footers = [], [], []\n",
    "    for txt in chosen_texts:\n",
    "        body, h, f = _strip_headers_footers(txt, headers, footers, window=4)\n",
    "        stripped.append(body); page_headers.append(h); page_footers.append(f)\n",
    "    paragraphs_per_page = [split_paragraphs_smart(t) for t in stripped]\n",
    "    out_path = Path(out_jsonl_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open('w', encoding='utf-8') as f:\n",
    "        for i in range(n_pages):\n",
    "            rec = {'source': str(Path(pdf_path).resolve()), 'page_no': i+1, 'method': methods[i], 'text': stripped[i], 'paragraphs': paragraphs_per_page[i]}\n",
    "            if page_headers[i]: rec['header'] = page_headers[i]\n",
    "            if page_footers[i]: rec['footer'] = page_footers[i]\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "    ocr_pages = len([m for m in methods if m == \"ocr\"])\n",
    "    print(f'Wrote JSONL: {out_path.resolve()} (pages={n_pages}, ocr_pages={ocr_pages}, ocr_dpi={ocr_dpi}, ocr_lang={\"dynamic\" if use_dynamic_lang and not (\"AKSONOCR_API_KEY\" in globals() and AKSONOCR_API_KEY) else (ocr_lang if not (\"AKSONOCR_API_KEY\" in globals() and AKSONOCR_API_KEY) else \"aksonocr\")})')\n",
    "    return str(out_path)\n",
    "\n",
    "# Excel/CSV ingestion → JSONL ต่อชีต และ .txt รวมทั้งไฟล์\n",
    "\n",
    "def _df_to_sheet_text(df) -> str:\n",
    "    # แปลงแถวเป็นบรรทัด: รวมแต่ละแถวด้วย \" | \" แล้วรวมทุกแถวด้วย \"\\n\"\n",
    "    if df is None or df.empty:\n",
    "        return ''\n",
    "    df = df.fillna('')\n",
    "    df = df.astype(str)\n",
    "    lines = [' | '.join(c.strip() for c in row if str(c).strip()) for row in df.values.tolist()]\n",
    "    lines = [ln for ln in lines if ln.strip()]\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def extract_excel_to_jsonl(xl_path: str, out_jsonl_path: str) -> str:\n",
    "    p = Path(xl_path)\n",
    "    out_path = Path(out_jsonl_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    texts_per_sheet = []\n",
    "    try:\n",
    "        if p.suffix.lower() in ['.csv', '.tsv']:\n",
    "            sep = '\\t' if p.suffix.lower() == '.tsv' else ','\n",
    "            try:\n",
    "                df = _pd.read_csv(p, sep=sep, encoding='utf-8-sig')\n",
    "            except Exception:\n",
    "                df = _pd.read_csv(p, sep=sep, encoding='cp874', errors='ignore')\n",
    "            sheets = {'CSV': df}\n",
    "        else:\n",
    "            sheets = _pd.read_excel(p, sheet_name=None, engine=None)\n",
    "    except Exception as e:\n",
    "        print(f'WARN: อ่านไฟล์ตารางไม่สำเร็จ {p.name}: {e}')\n",
    "        sheets = {}\n",
    "\n",
    "    with out_path.open('w', encoding='utf-8') as f:\n",
    "        for idx, (sheet_name, df) in enumerate(sheets.items(), start=1):\n",
    "            raw = _df_to_sheet_text(df)\n",
    "            clean = clean_for_index(raw)\n",
    "            paras = split_paragraphs_smart(clean)\n",
    "            rec = {\n",
    "                'source': str(p.resolve()),\n",
    "                'page_no': idx,            # map ชีต -> page_no ให้ chunker ใช้งานต่อได้\n",
    "                'sheet': str(sheet_name),\n",
    "                'method': 'excel',\n",
    "                'text': clean,\n",
    "                'paragraphs': paras,\n",
    "            }\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "            texts_per_sheet.append(clean)\n",
    "\n",
    "    print(f'Wrote JSONL (excel): {out_path.resolve()} (sheets={len(texts_per_sheet)})')\n",
    "    txt_out = out_path.with_suffix('.txt') if out_path.name.endswith('.jsonl') else (out_path.parent / (p.stem + '.txt'))\n",
    "    txt_out.write_text('\\n\\n'.join(texts_per_sheet), encoding='utf-8')\n",
    "    return str(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70fe26e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 PDF(s) and 2 table file(s) in C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Source\n",
      "\n",
      "Processing PDF: Source\\129.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\129.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\131.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\131.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\133.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\133.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\135.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\135.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\137.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\137.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\139.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\139.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\2018_06_08_15_10_33.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\2018_06_08_15_10_33.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\2018_06_08_15_13_16.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\2018_06_08_15_13_16.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\2568THV3-5TH.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\2568THV3-5TH.jsonl (pages=3, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\AcademicCalendar2025TH.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\AcademicCalendar2025TH.jsonl (pages=6, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\ad_out57.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\ad_out57.jsonl (pages=3, ocr_pages=3, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\announce_financ.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\announce_financ.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\announce_financCancel.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\announce_financCancel.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\anounc_move52ene.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\anounc_move52ene.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\Approved-exam2568.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\Approved-exam2568.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\calculator2023-2.pdf\n",
      "[page 1] OCR lang override: tha -> tha+eng\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\calculator2023-2.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\calculator2023.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\calculator2023.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\celemony2539.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\celemony2539.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\COVID-19 (2).pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\COVID-19 (2).jsonl (pages=3, ocr_pages=3, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\develop_eng-2563covid19.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\develop_eng-2563covid19.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\develop_eng-2564covid19.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\develop_eng-2564covid19.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\discipline2566_fulltext.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\discipline2566_fulltext.jsonl (pages=11, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\duplicate2551.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\duplicate2551.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\ENG-B2568.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\ENG-B2568.jsonl (pages=4, ocr_pages=4, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\ENG-D2568.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\ENG-D2568.jsonl (pages=5, ocr_pages=5, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\ENG-M2568.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\ENG-M2568.jsonl (pages=4, ocr_pages=4, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\ENG2561.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\ENG2561.jsonl (pages=3, ocr_pages=3, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\eng2564B.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\eng2564B.jsonl (pages=4, ocr_pages=4, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\eng2564D.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\eng2564D.jsonl (pages=4, ocr_pages=4, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\eng2564M.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\eng2564M.jsonl (pages=4, ocr_pages=4, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\English_Grad_2017.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\English_Grad_2017.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\etc1.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\etc1.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\fee2567update.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\fee2567update.jsonl (pages=1, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\FOE10_วศ.บ.วิศวกรรมคอมพิวเตอร์_2564.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\FOE10_วศ.บ.วิศวกรรมคอมพิวเตอร์_2564.jsonl (pages=34, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\handbook2562g.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\handbook2562g.jsonl (pages=32, ocr_pages=32, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\industraltranning2563update.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\industraltranning2563update.jsonl (pages=2, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\insurance-inter-std.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\insurance-inter-std.jsonl (pages=3, ocr_pages=3, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\insurance-std (1).pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\insurance-std (1).jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\insurance-std.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\insurance-std.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\IP2565.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\IP2565.jsonl (pages=9, ocr_pages=9, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\new_2558.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\new_2558.jsonl (pages=2, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\OBEM.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\OBEM.jsonl (pages=3, ocr_pages=3, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\OBEM2566update.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\OBEM2566update.jsonl (pages=2, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\pre-co-obem2567.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\pre-co-obem2567.jsonl (pages=1, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\Pre-requisite2567-final.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\Pre-requisite2567-final.jsonl (pages=2, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\price.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\price.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\privacy2563.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\privacy2563.jsonl (pages=7, ocr_pages=7, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\rule2563-2g.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\rule2563-2g.jsonl (pages=1, ocr_pages=1, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\rule2563-4b.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\rule2563-4b.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\rule57.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\rule57.jsonl (pages=21, ocr_pages=21, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\rule57_2.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\rule57_2.jsonl (pages=6, ocr_pages=6, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\rule_covid2564.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\rule_covid2564.jsonl (pages=3, ocr_pages=3, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\rule_exam2560.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\rule_exam2560.jsonl (pages=5, ocr_pages=5, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\ruleG2568.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\ruleG2568.jsonl (pages=30, ocr_pages=30, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\schedule2565.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\schedule2565.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\t_fee.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\t_fee.jsonl (pages=2, ocr_pages=2, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\tetet2562doctor.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\tetet2562doctor.jsonl (pages=3, ocr_pages=3, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\tf_out.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\tf_out.jsonl (pages=3, ocr_pages=3, ocr_dpi=450, ocr_lang=dynamic)\n",
      "Full-file OCR lang = tha\n",
      "\n",
      "Processing PDF: Source\\ประกาศ_มจธ_หลักเกณฑ์การจัดสรรผลประโยชน์_พศ2566_ฉบับเต็ม.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\ประกาศ_มจธ_หลักเกณฑ์การจัดสรรผลประโยชน์_พศ2566_ฉบับเต็ม.jsonl (pages=4, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\วศ.บ.-วศวกรรมคอมพวเตอร-ปรบปรง.64.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\วศ.บ.-วศวกรรมคอมพวเตอร-ปรบปรง.64.jsonl (pages=237, ocr_pages=33, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing PDF: Source\\โครงสร้างหลักสูตรรายวิชาศึกษาทั่วไป มจธ.pdf\n",
      "Wrote JSONL: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\โครงสร้างหลักสูตรรายวิชาศึกษาทั่วไป มจธ.jsonl (pages=27, ocr_pages=0, ocr_dpi=450, ocr_lang=dynamic)\n",
      "\n",
      "Processing EXCEL: Source\\data_Form_KMUTT.xlsx\n",
      "Wrote JSONL (excel): C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\data_Form_KMUTT.jsonl (sheets=1)\n",
      "\n",
      "Processing EXCEL: Source\\kmutt_all_boards.xlsx\n",
      "Wrote JSONL (excel): C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\kmutt_all_boards.jsonl (sheets=1)\n",
      "\n",
      "Done.\n",
      "- 129.pdf: 3098 chars\n",
      "- 131.pdf: 4012 chars\n",
      "- 133.pdf: 2704 chars\n",
      "- 135.pdf: 2102 chars\n",
      "- 137.pdf: 1052 chars\n",
      "- 139.pdf: 1033 chars\n",
      "- 2018_06_08_15_10_33.pdf: 2038 chars\n",
      "- 2018_06_08_15_13_16.pdf: 4197 chars\n",
      "- 2568THV3-5TH.pdf: 5142 chars\n",
      "- AcademicCalendar2025TH.pdf: 12016 chars\n",
      "- ad_out57.pdf: 4064 chars\n",
      "- announce_financ.pdf: 2393 chars\n",
      "- announce_financCancel.pdf: 1494 chars\n",
      "- anounc_move52ene.pdf: 1857 chars\n",
      "- Approved-exam2568.pdf: 1218 chars\n",
      "- calculator2023-2.pdf: 12 chars\n",
      "- calculator2023.pdf: 1777 chars\n",
      "- celemony2539.pdf: 2155 chars\n",
      "- COVID-19 (2).pdf: 6905 chars\n",
      "- develop_eng-2563covid19.pdf: 3093 chars\n",
      "- develop_eng-2564covid19.pdf: 3321 chars\n",
      "- discipline2566_fulltext.pdf: 19881 chars\n",
      "- duplicate2551.pdf: 1900 chars\n",
      "- ENG-B2568.pdf: 6779 chars\n",
      "- ENG-D2568.pdf: 7886 chars\n",
      "- ENG-M2568.pdf: 7108 chars\n",
      "- ENG2561.pdf: 4248 chars\n",
      "- eng2564B.pdf: 4709 chars\n",
      "- eng2564D.pdf: 4253 chars\n",
      "- eng2564M.pdf: 4244 chars\n",
      "- English_Grad_2017.pdf: 2561 chars\n",
      "- etc1.pdf: 1315 chars\n",
      "- fee2567update.pdf: 2309 chars\n",
      "- FOE10_วศ.บ.วิศวกรรมคอมพิวเตอร์_2564.pdf: 50368 chars\n",
      "- handbook2562g.pdf: 69188 chars\n",
      "- industraltranning2563update.pdf: 3130 chars\n",
      "- insurance-inter-std.pdf: 4205 chars\n",
      "- insurance-std (1).pdf: 3323 chars\n",
      "- insurance-std.pdf: 3323 chars\n",
      "- IP2565.pdf: 20403 chars\n",
      "- new_2558.pdf: 2430 chars\n",
      "- OBEM.pdf: 4828 chars\n",
      "- OBEM2566update.pdf: 5715 chars\n",
      "- pre-co-obem2567.pdf: 1750 chars\n",
      "- Pre-requisite2567-final.pdf: 2320 chars\n",
      "- price.pdf: 1641 chars\n",
      "- privacy2563.pdf: 13098 chars\n",
      "- rule2563-2g.pdf: 1391 chars\n",
      "- rule2563-4b.pdf: 2279 chars\n",
      "- rule57.pdf: 40171 chars\n",
      "- rule57_2.pdf: 15632 chars\n",
      "- rule_covid2564.pdf: 5313 chars\n",
      "- rule_exam2560.pdf: 10699 chars\n",
      "- ruleG2568.pdf: 74497 chars\n",
      "- schedule2565.pdf: 3710 chars\n",
      "- t_fee.pdf: 2199 chars\n",
      "- tetet2562doctor.pdf: 3469 chars\n",
      "- tf_out.pdf: 4540 chars\n",
      "- ประกาศ_มจธ_หลักเกณฑ์การจัดสรรผลประโยชน์_พศ2566_ฉบับเต็ม.pdf: 4731 chars\n",
      "- วศ.บ.-วศวกรรมคอมพวเตอร-ปรบปรง.64.pdf: 312709 chars\n",
      "- โครงสร้างหลักสูตรรายวิชาศึกษาทั่วไป มจธ.pdf: 50548 chars\n",
      "- data_Form_KMUTT.xlsx: 4212 chars\n",
      "- kmutt_all_boards.xlsx: 17318 chars\n"
     ]
    }
   ],
   "source": [
    "# Batch runner: ประมวลผลทุกไฟล์ PDF และ Excel/CSV ในโฟลเดอร์\n",
    "INPUT_DIR = Path('Source')          # ใช้โฟลเดอร์ฐานข้อมูลที่ให้มา\n",
    "OUTPUT_DIR = Path('Database')       # โฟลเดอร์ผลลัพธ์ฐานข้อมูลรวม\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# เพิ่มตัวช่วยทำความสะอาดข้อความสำหรับการทำดัชนี\n",
    "def clean_for_index(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ทำความสะอาดข้อความก่อนเขียนออก/ทำดัชนี:\n",
    "    - normalize unicode และเว้นบรรทัด\n",
    "    - รวมคำที่ถูกตัดด้วย hyphen ตอนขึ้นบรรทัดใหม่ (ภาษาอังกฤษ/ตัวเลข)\n",
    "    - ลดช่องว่างซ้ำและบรรทัดว่างเกินจำเป็น\n",
    "    - tidy ภาษาไทย (เว้นวรรค/normalize)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return ''\n",
    "    t = normalize_text(text, preserve_newlines=True)\n",
    "    # รวมคำแบบ hyphen-break เฉพาะอักษรละติน/ตัวเลข\n",
    "    t = re.sub(r'([A-Za-z0-9])-\\n([A-Za-z0-9])', r'\\1\\2', t)\n",
    "    # ตัดช่องว่างหัวท้ายของแต่ละบรรทัด\n",
    "    t = '\\n'.join(ln.strip() for ln in t.split('\\n'))\n",
    "    # ลดบรรทัดว่างติดกัน\n",
    "    t = re.sub(r'\\n{3,}', '\\n\\n', t)\n",
    "    # tidy ภาษาไทย\n",
    "    t = thai_postprocess(t)\n",
    "    return t.strip()\n",
    "\n",
    "# ตั้งค่าภาษาและ DPI สำหรับ OCR เท่านั้น (MuPDF ยังใช้ได้ตามปกติ)\n",
    "OCR_LANG = 'tha'     # ตั้งค่าเริ่มต้นเป็นไทย; ระบบจะสลับเป็น 'tha+eng' รายหน้าอัตโนมัติถ้าพบอักษรอังกฤษมากพอ\n",
    "OCR_DPI = 450        # ปรับเป็น 450/500 หากจำเป็น\n",
    "\n",
    "pdf_files = sorted([p for p in INPUT_DIR.glob('**/*.pdf')])\n",
    "excel_globs = ['**/*.xlsx', '**/*.xls', '**/*.csv', '**/*.tsv']\n",
    "excel_files = []\n",
    "for patt in excel_globs:\n",
    "    excel_files.extend(INPUT_DIR.glob(patt))\n",
    "excel_files = sorted(set(excel_files))\n",
    "\n",
    "print(f'Found {len(pdf_files)} PDF(s) and {len(excel_files)} table file(s) in {INPUT_DIR.resolve()}')\n",
    "\n",
    "results = []\n",
    "\n",
    "# PDFs → JSONL per-page + .txt\n",
    "for pdf in pdf_files:\n",
    "    print(f'\\nProcessing PDF: {pdf}')\n",
    "    # 1) JSONL per-page\n",
    "    jsonl_path = OUTPUT_DIR / (pdf.stem + '.jsonl')\n",
    "    extract_pages_with_fallback_to_jsonl(\n",
    "        pdf_path=str(pdf),\n",
    "        out_jsonl_path=str(jsonl_path),\n",
    "        dpi=300,\n",
    "        ocr_lang=OCR_LANG,\n",
    "        ocr_dpi=OCR_DPI,\n",
    "        min_length=50,\n",
    "        min_score=0.2,\n",
    "        poppler_bin=POPPLER_BIN,\n",
    "        use_dynamic_lang=True,  # เปิดโหมดเลือกภาษาอัตโนมัติรายหน้า\n",
    "    )\n",
    "    # 2) รวมข้อความทั้งไฟล์ และบันทึก .txt (เพื่ออ่านง่าย)\n",
    "    raw = extract_text_mupdf(str(pdf))\n",
    "    if not raw.strip():\n",
    "        if 'AKSONOCR_API_KEY' in globals() and AKSONOCR_API_KEY:\n",
    "            print('Using AksonOCR for full-file OCR…')\n",
    "            raw = ocr_pdf_akson(str(pdf), dpi=OCR_DPI, api_key=AKSONOCR_API_KEY, poppler_bin=POPPLER_BIN)\n",
    "        else:\n",
    "            # เลือกภาษา OCR ระดับไฟล์: ดูจาก 2-3 หน้าแรก ถ้ามีอักษรละตินมากพอ → tha+eng\n",
    "            try:\n",
    "                kwargs = {}\n",
    "                if (POPPLER_BIN) and os.name == 'nt':\n",
    "                    kwargs['poppler_path'] = POPPLER_BIN\n",
    "                _ = convert_from_path(str(pdf), dpi=200, first_page=1, last_page=min(3, 9999), **kwargs)\n",
    "                preview_text = extract_text_mupdf(str(pdf))[:5000]\n",
    "                doc_lang = choose_ocr_lang_for_text(preview_text, default='tha', latin_threshold=0.15)\n",
    "            except Exception:\n",
    "                doc_lang = OCR_LANG\n",
    "            print(f'Full-file OCR lang = {doc_lang}')\n",
    "            raw = ocr_pdf(str(pdf), dpi=OCR_DPI, lang=doc_lang, poppler_bin=POPPLER_BIN)\n",
    "    clean = clean_for_index(raw)\n",
    "    txt_out = OUTPUT_DIR / (pdf.stem + '.txt')\n",
    "    txt_out.write_text(clean, encoding='utf-8')\n",
    "    results.append((pdf.name, len(clean)))\n",
    "\n",
    "# Excel/CSV → JSONL per-sheet + .txt\n",
    "for xf in excel_files:\n",
    "    print(f'\\nProcessing EXCEL: {xf}')\n",
    "    jsonl_path = OUTPUT_DIR / (xf.stem + '.jsonl')\n",
    "    extract_excel_to_jsonl(str(xf), str(jsonl_path))\n",
    "    try:\n",
    "        total_txt = (OUTPUT_DIR / (xf.stem + '.txt')).read_text(encoding='utf-8')\n",
    "        results.append((xf.name, len(total_txt)))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print('\\nDone.')\n",
    "for name, n in results:\n",
    "    print(f'- {name}: {n} chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1b0442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregator: รวม JSONL ทั้งหมดเป็น index.jsonl และสร้าง manifest.json\\nfrom datetime import datetime\\n\\njsonl_files = sorted(OUTPUT_DIR.glob('*.jsonl'))\\nindex_path = OUTPUT_DIR / 'index.jsonl'\\nmanifest_path = OUTPUT_DIR / 'manifest.json'\\n\\n# รวมไฟล์ JSONL เข้าด้วยกัน พร้อมเติมข้อมูลไฟล์ต้นทาง\\nrecords = 0\\nwith index_path.open('w', encoding='utf-8') as out_f:\\n    for jf in jsonl_files:\\n        try:\\n            for line in jf.read_text(encoding='utf-8').splitlines():\\n                if not line.strip():\\n                    continue\\n                obj = json.loads(line)\\n                # enrich metadata\\n                obj['file_jsonl'] = str(jf.resolve())\\n                obj['file_name'] = jf.stem\\n                out_f.write(json.dumps(obj, ensure_ascii=False) + '\\n')\\n                records += 1\\n        except Exception as e:\\n            print(f'WARN: skip {jf.name}: {e}')\\n\\n# สร้าง manifest ของฐานข้อมูล\\nmanifest = {\\n    'created_at': datetime.utcnow().isoformat() + 'Z',\\n    'input_dir': str(INPUT_DIR.resolve()),\\n    'output_dir': str(OUTPUT_DIR.resolve()),\\n    'files_count': len(jsonl_files),\\n    'records': records,\\n    'notes': 'index.jsonl รวมทุกหน้า ทุกเอกสาร; แต่ละบรรทัดมี fields เช่น source(ไฟล์ PDF), page_no, method, text, paragraphs',\\n}\\nmanifest_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding='utf-8')\\nprint(f'Wrote index: {index_path.resolve()} (records={records})')\\nprint(f'Wrote manifest: {manifest_path.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9f4b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote chunks: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\chunks.jsonl (chunks=1213)\n"
     ]
    }
   ],
   "source": [
    "# Semantic-first chunker: จาก Database/index.jsonl → Database/chunks.jsonl\n",
    "# เป้าหมาย: ชิ้นข้อความ 400–800 โทเคน (ไทย ~ 1 โทเคน ≈ 4 ตัวอักษร), overlap 10–15%, รักษาโครงสร้าง\n",
    "import math, json, re, time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# ค่าตั้งต้น (ปรับได้)\n",
    "TARGET_MIN_TOKENS = 400\n",
    "TARGET_MAX_TOKENS = 800\n",
    "OVERLAP_RATIO     = 0.12   # 12%\n",
    "SIM_BREAK_THRESHOLD = 0.5   # ทางเลือก (ยังไม่ใช้ embedding หนัก)\n",
    "CHAR_PER_TOKEN    = 4.0     # ไทย: ประมาณ 1 โทเคน ≈ 4 ตัวอักษร\n",
    "\n",
    "# Metadata เริ่มต้นสำหรับ RAG/ACL\n",
    "DEFAULT_OWNER = \"owner:unknown\"\n",
    "DEFAULT_SENSITIVITY = \"internal\"  # public|internal|confidential\n",
    "\n",
    "# ยูทิลนับโทเคนคร่าว ๆ (ไทย)\n",
    "def est_tokens(text: str) -> int:\n",
    "    return max(1, int(math.ceil(len(text) / CHAR_PER_TOKEN)))\n",
    "\n",
    "# แปลงชื่อไฟล์/เอกสารให้เป็นรูปแบบ normalized (เช่น policy_2024.txt)\n",
    "def normalize_doc_name(src_path: str) -> str:\n",
    "    name = Path(src_path).stem.lower()\n",
    "    # อนุญาต a-z0-9 และตัวอักษรไทย; ที่เหลือแทนด้วย _\n",
    "    name = re.sub(r\"[^0-9A-Za-z\\u0E00-\\u0E7F]+\", \"_\", name).strip(\"_\")\n",
    "    if not name:\n",
    "        name = \"document\"\n",
    "    if not name.endswith(\".txt\"):\n",
    "        name = f\"{name}.txt\"\n",
    "    return name\n",
    "\n",
    "# ตรวจหัวข้อ/หัวเรื่อง\n",
    "_HEADING_PATTS = [\n",
    "    r\"^บท\\s*ที่\\s*\\d+\", r\"^หมวด\\s*ที่?\\s*\\d+\", r\"^ภาคผนวก\", r\"^บท\\s*\\d+\",\n",
    "    r\"^(?:\\d+\\.)+\\s+\",  # 1. , 1.1. , 2.3.4.\n",
    "    r\"^\\d+\\)\\s+\",       # 1)\n",
    "    r\"^[A-Za-zก-๙]+\\s*:\\s+\",  # Topic:\n",
    "]\n",
    "_HEADING_RE = re.compile(\"|\".join(_HEADING_PATTS))\n",
    "\n",
    "# ตรวจรายการ (bullet/ข้อ/อนุข้อ)\n",
    "_BULLET_PATTS = [\n",
    "    r\"^[\\-\\•\\–]\\s+\", r\"^\\u0E51\\.|^๑\\.\",  # - , • , – , ๑.\n",
    "    r\"^[ก-ฮ]\\)\\s+\",    # ก) ข) ค)\n",
    "    r\"^\\([ก-ฮ]\\)\\s+\", r\"^\\([0-9]+\\)\\s+\",\n",
    "]\n",
    "_BULLET_RE = re.compile(\"|\".join(_BULLET_PATTS))\n",
    "\n",
    "# แบ่งประโยคแบบคร่าว ๆ (ไทย/อังกฤษปน)\n",
    "_SENT_SPLIT_RE = re.compile(r\"(?<=[\\.!?…\\u0E2F\\u0E5B\\u0E46\\u0E2E])\\s+\")\n",
    "\n",
    "def is_heading(text: str) -> bool:\n",
    "    t = text.strip()\n",
    "    return bool(_HEADING_RE.search(t))\n",
    "\n",
    "def is_bullet(text: str) -> bool:\n",
    "    t = text.strip()\n",
    "    return bool(_BULLET_RE.search(t))\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    # พยายามตัดตามเครื่องหมายวรรคตอนก่อน ถ้าไม่ได้ผลค่อย fallback\n",
    "    t = text.strip()\n",
    "    parts = _SENT_SPLIT_RE.split(t)\n",
    "    if len(parts) > 1:\n",
    "        return [p.strip() for p in parts if p.strip()]\n",
    "    # fallback: ตัดทุก ~300–500 ตัวอักษรเพื่อเลี่ยงชิ้นยาวเกินไป (ไทยไม่มีช่องว่าง)\n",
    "    max_chars = int(TARGET_MAX_TOKENS * CHAR_PER_TOKEN * 0.6)\n",
    "    if max_chars < 300:\n",
    "        max_chars = 300\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(t):\n",
    "        out.append(t[i:i+max_chars])\n",
    "        i += max_chars\n",
    "    return out\n",
    "\n",
    "def group_bullets(paragraphs: list[dict]) -> list[dict]:\n",
    "    \"\"\"รวมบรรทัด bullet ติดกันเป็นบล็อกเดียว เพื่อเลี่ยงการตัดกลางข้อ\"\"\"\n",
    "    grouped = []\n",
    "    buf = []\n",
    "    for p in paragraphs:\n",
    "        if is_bullet(p['text']):\n",
    "            buf.append(p)\n",
    "        else:\n",
    "            if buf:\n",
    "                # รวม bullets ก่อนหน้าเป็นหนึ่งย่อหน้า\n",
    "                merged = {**buf[0]}\n",
    "                merged['text'] = \"\\n\".join(x['text'] for x in buf)\n",
    "                grouped.append(merged)\n",
    "                buf = []\n",
    "            grouped.append(p)\n",
    "    if buf:\n",
    "        merged = {**buf[0]}\n",
    "        merged['text'] = \"\\n\".join(x['text'] for x in buf)\n",
    "        grouped.append(merged)\n",
    "    return grouped\n",
    "\n",
    "def load_index(index_path: Path) -> list[dict]:\n",
    "    recs = []\n",
    "    with index_path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            recs.append(json.loads(line))\n",
    "    # ควรเรียงตาม source, page_no เพื่อคงลำดับ\n",
    "    recs.sort(key=lambda r: (r.get('source',''), r.get('page_no', 0)))\n",
    "    return recs\n",
    "\n",
    "def paragraphs_from_records(recs: list[dict]) -> list[dict]:\n",
    "    \"\"\"สร้างลิสต์ย่อหน้าโดยคง page_no และตรวจหัวข้อ\"\"\"\n",
    "    out = []\n",
    "    for r in recs:\n",
    "        page_raw = r.get('page_no')\n",
    "        try:\n",
    "            page = int(page_raw) if page_raw is not None else 0\n",
    "        except (ValueError, TypeError):\n",
    "            page = 0\n",
    "        paras = r.get('paragraphs') or [r.get('text','')]\n",
    "        for t in paras:\n",
    "            if not t or not t.strip():\n",
    "                continue\n",
    "            out.append({\n",
    "                'page': page,\n",
    "                'text': t.strip(),\n",
    "                'is_heading': is_heading(t),\n",
    "            })\n",
    "    return group_bullets(out)\n",
    "\n",
    "def make_chunks(paragraphs: list[dict], source_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    เขียนชิ้นข้อมูลด้วยเมทาดาทาตามสคีมาแนะนำ:\n",
    "    - source: ชื่อไฟล์/ชื่อเอกสารหลัง normalize (เช่น policy_2024.txt)\n",
    "    - path  : พาธเต็มของไฟล์บนระบบ\n",
    "    - page  : หน้าอ้างอิงหลัก (ถ้าคร่อมหลายหน้า ให้เป็นหน้าแรก)\n",
    "    - owner : เจ้าของเอกสาร/ทีม\n",
    "    - sensitivity: public|internal|confidential\n",
    "    - updated_at: unix epoch (วินาที)\n",
    "    - page_start/page_end: ระบุช่วงหน้า (ยังคง page = page_start)\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    cur_texts = []\n",
    "    cur_pages = []\n",
    "    cur_tokens = 0\n",
    "    cur_section = None\n",
    "\n",
    "    src_name = normalize_doc_name(source_path)\n",
    "    full_path = str(Path(source_path).resolve())\n",
    "\n",
    "    def add_paragraph(p):\n",
    "        nonlocal cur_tokens\n",
    "        cur_texts.append(p['text'])\n",
    "        cur_pages.append(p['page'])\n",
    "        cur_tokens += est_tokens(p['text'])\n",
    "\n",
    "    def finalize_chunk(overlap_tail: str | None = None):\n",
    "        nonlocal cur_texts, cur_pages, cur_tokens\n",
    "        if not cur_texts:\n",
    "            return\n",
    "        text = (overlap_tail + \"\\n\" if overlap_tail else \"\") + \"\\n\\n\".join(cur_texts).strip()\n",
    "        # ป้องกันกรณี page_no เป็น None หรือไม่ใช่ตัวเลข\n",
    "        valid_pages = []\n",
    "        for pg in cur_pages:\n",
    "            try:\n",
    "                if pg is not None:\n",
    "                    valid_pages.append(int(pg))\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        page_start = min(valid_pages) if valid_pages else 0\n",
    "        page_end   = max(valid_pages) if valid_pages else 0\n",
    "        chunk = {\n",
    "            'source': src_name,\n",
    "            'path': full_path,\n",
    "            'page': page_start,\n",
    "            'page_start': page_start,\n",
    "            'page_end': page_end,\n",
    "            'owner': DEFAULT_OWNER,\n",
    "            'sensitivity': DEFAULT_SENSITIVITY,\n",
    "            'updated_at': int(time.time()),  # unix epoch seconds\n",
    "            'text': text,\n",
    "            'tokens_est': est_tokens(text),\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "        cur_texts = []\n",
    "        cur_pages = []\n",
    "        cur_tokens = 0\n",
    "\n",
    "    i = 0\n",
    "    n = len(paragraphs)\n",
    "    while i < n:\n",
    "        p = paragraphs[i]\n",
    "        # พบหัวข้อ: ปิดชิ้นก่อนหน้า (ถ้าชิ้นมีเนื้อหา) เพื่อรักษาโครงสร้าง\n",
    "        if p['is_heading']:\n",
    "            if cur_texts:\n",
    "                text_joined = \"\\n\\n\".join(cur_texts)\n",
    "                overlap_chars = int(OVERLAP_RATIO * len(text_joined))\n",
    "                tail = text_joined[-overlap_chars:] if overlap_chars > 0 else None\n",
    "                finalize_chunk()\n",
    "                carry_tail = tail\n",
    "            else:\n",
    "                carry_tail = None\n",
    "            cur_section = p['text']\n",
    "            add_paragraph(p)\n",
    "            # หากหัวข้อยาวมาก ให้ซอยด้วยประโยค\n",
    "            if cur_tokens > TARGET_MAX_TOKENS:\n",
    "                sents = split_sentences(p['text'])\n",
    "                cur_texts = []\n",
    "                cur_pages = [p['page']]\n",
    "                cur_tokens = 0\n",
    "                buf = []\n",
    "                for s in sents:\n",
    "                    if est_tokens(\"\\n\".join(buf + [s])) > TARGET_MAX_TOKENS:\n",
    "                        finalize_chunk(carry_tail)\n",
    "                        carry_tail = None\n",
    "                        buf = [s]\n",
    "                    else:\n",
    "                        buf.append(s)\n",
    "                if buf:\n",
    "                    joined = \" \".join(buf)\n",
    "                    cur_texts = [joined]\n",
    "                    cur_tokens = est_tokens(joined)\n",
    "            # บันทึก tail เมื่อ finalize ครั้งถัดไป\n",
    "            _pending_tail = carry_tail\n",
    "            def _finalize_with_pending():\n",
    "                nonlocal _pending_tail\n",
    "                finalize_chunk(_pending_tail)\n",
    "                _pending_tail = None\n",
    "            finalize_current = _finalize_with_pending  # not used elsewhere but kept for clarity\n",
    "        else:\n",
    "            # ย่อหน้าทั่วไป\n",
    "            if cur_tokens + est_tokens(p['text']) <= TARGET_MAX_TOKENS:\n",
    "                add_paragraph(p)\n",
    "            else:\n",
    "                # เกิน TARGET_MAX → ปิดชิ้นที่มีอยู่ พร้อม overlap\n",
    "                text_joined = \"\\n\\n\".join(cur_texts)\n",
    "                overlap_chars = int(OVERLAP_RATIO * len(text_joined))\n",
    "                tail = text_joined[-overlap_chars:] if overlap_chars > 0 else None\n",
    "                finalize_chunk(tail)\n",
    "                # เริ่มชิ้นใหม่ด้วยย่อหน้าปัจจุบัน ถ้ายังยาวไป ให้ซอยเป็นประโยค\n",
    "                if est_tokens(p['text']) > TARGET_MAX_TOKENS:\n",
    "                    sents = split_sentences(p['text'])\n",
    "                    buf = []\n",
    "                    for s in sents:\n",
    "                        if est_tokens(\"\\n\".join(buf + [s])) > TARGET_MAX_TOKENS:\n",
    "                            finalize_chunk()  # ไม่มี overlap ในกลางย่อหน้า\n",
    "                            buf = [s]\n",
    "                        else:\n",
    "                            buf.append(s)\n",
    "                    if buf:\n",
    "                        joined = \" \".join(buf)\n",
    "                        cur_texts = [joined]\n",
    "                        cur_pages = [p['page']]\n",
    "                        cur_tokens = est_tokens(joined)\n",
    "                    else:\n",
    "                        cur_texts, cur_pages, cur_tokens = [], [], 0\n",
    "                else:\n",
    "                    cur_texts, cur_pages, cur_tokens = [p['text']], [p['page']], est_tokens(p['text'])\n",
    "        i += 1\n",
    "\n",
    "    # ปิดชิ้นสุดท้าย\n",
    "    finalize_chunk()\n",
    "    return chunks\n",
    "\n",
    "# สร้าง chunks.jsonl จาก index.jsonl\n",
    "INDEX_PATH = OUTPUT_DIR / 'index.jsonl'\n",
    "CHUNKS_PATH = OUTPUT_DIR / 'chunks.jsonl'\n",
    "\n",
    "# Fallback: ถ้า index.jsonl ยังไม่มี ให้รวมจากไฟล์ .jsonl ต่อไฟล์ก่อน (เหมือน aggregator ย่อส่วน)\n",
    "def _build_index_if_missing(output_dir: Path, index_path: Path) -> int:\n",
    "    if index_path.exists():\n",
    "        return 0\n",
    "    jsonl_files = [p for p in sorted(output_dir.glob('*.jsonl')) if p.name not in ('index.jsonl', 'manifest.json')]\n",
    "    records = 0\n",
    "    index_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with index_path.open('w', encoding='utf-8') as out_f:\n",
    "        for jf in jsonl_files:\n",
    "            try:\n",
    "                for line in jf.read_text(encoding='utf-8').splitlines():\n",
    "                    if not line.strip():\n",
    "                        continue\n",
    "                    obj = json.loads(line)\n",
    "                    obj['file_jsonl'] = str(jf.resolve())\n",
    "                    obj['file_name']  = jf.stem\n",
    "                    out_f.write(json.dumps(obj, ensure_ascii=False) + '\\n')\n",
    "                    records += 1\n",
    "            except Exception as e:\n",
    "                print(f'WARN: skip {jf.name}: {e}')\n",
    "    if records:\n",
    "        print(f'Built missing index.jsonl (records={records})')\n",
    "    return records\n",
    "\n",
    "# สร้าง index ถ้ายังไม่มี\n",
    "_build_index_if_missing(OUTPUT_DIR, INDEX_PATH)\n",
    "\n",
    "if INDEX_PATH.exists():\n",
    "    recs = load_index(INDEX_PATH)\n",
    "    # จัดกลุ่มตามไฟล์ (source)\n",
    "    by_src = {}\n",
    "    for r in recs:\n",
    "        by_src.setdefault(r.get('source','unknown'), []).append(r)\n",
    "    all_chunks = []\n",
    "    for src, items in by_src.items():\n",
    "        paras = paragraphs_from_records(items)\n",
    "        chunks = make_chunks(paras, src)\n",
    "        all_chunks.extend(chunks)\n",
    "    # เขียนออกเป็น JSONL\n",
    "    with CHUNKS_PATH.open('w', encoding='utf-8') as f:\n",
    "        for ch in all_chunks:\n",
    "            f.write(json.dumps(ch, ensure_ascii=False) + '\\n')\n",
    "    print(f'Wrote chunks: {CHUNKS_PATH.resolve()} (chunks={len(all_chunks)})')\n",
    "else:\n",
    "    print('index.jsonl not found and could not be built. Run the batch and aggregator steps first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ea277",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "1) วางไฟล์ PDF ในโฟลเดอร์ `Source` (รองรับซับโฟลเดอร์)\n",
    "2) รันเซลล์ตามลำดับจนถึง Aggregator (จะได้ `Database/index.jsonl` และ `Database/manifest.json`)\n",
    "3) รันเซลล์ Semantic-first chunker (จะได้ `Database/chunks.jsonl`) พร้อม metadata: source/path, page_start/page_end, section_title, owner, sensitivity, updated_at, allowed_groups\n",
    "\n",
    "ปรับแต่งได้:\n",
    "- OCR: ตั้ง `OCR_LANG` เป็น 'tha+eng' หรือ 'tha'; ตั้ง `OCR_DPI` เป็น 400–450 เมื่อคุณภาพแย่\n",
    "- Chunking: ตั้ง `TARGET_MIN_TOKENS`/`TARGET_MAX_TOKENS` (แนะนำ 400–800), `OVERLAP_RATIO` (10–15%)\n",
    "- โครงสร้าง: ตัว chunker พยายามรักษาหัวข้อ/รายการ และเลี่ยงตัดกลางข้อ; ถ้ายาวเกินจะซอยที่ย่อหน้า/ประโยคแบบค่อยเป็นค่อยไป\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f471b3",
   "metadata": {},
   "source": [
    "## Indexing: Embeddings (BAAI/bge-m3) → Qdrant + BM25 (Local) + Hybrid (RRF)\n",
    "\n",
    "> สรุปเวิร์กโฟลว์พร้อมรัน:\n",
    "- โหลดชิ้นข้อมูลจาก `Database/chunks.jsonl` หรือ `output/chunks/*.jsonl`\n",
    "- ฝั่งเวกเตอร์: สร้าง embedding ด้วย `BAAI/bge-m3` แล้วอัปโหลดเข้า Qdrant\n",
    "- ฝั่งคีย์เวิร์ด: สร้าง Local BM25 (rank-bm25) และบันทึกไว้ที่ `Database/bm25_index.pkl`\n",
    "- Hybrid Retrieval: รวมผล Vector + BM25 ด้วย Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "> ตั้งค่าได้ผ่าน ENV หรือแก้ในโค้ด:\n",
    "- Qdrant: `QDRANT_URL` (เช่น http://127.0.0.1:6333), `QDRANT_API_KEY` (ถ้ามี), `QDRANT_COLLECTION`\n",
    "- Embedding model: `EMB_MODEL` (เช่น BAAI/bge-m3)\n",
    "- Hybrid: `RRF_K` (ค่าคงที่ในสูตร RRF; ค่าเริ่มต้น 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c9ac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunks: 1213\n",
      "Encoding embeddings…\n",
      "Encoding embeddings…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 19/19 [12:27<00:00, 39.33s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim = 1024\n",
      "Upserting to Qdrant…\n",
      "Upserting to Qdrant…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1213/1213 [00:10<00:00, 110.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant upsert done.\n",
      "Tokenizer = thai-newmm\n",
      "Tokenizer = thai-newmm\n",
      "Building BM25 index…\n",
      "Saved Local BM25 index to: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\bm25_index.pkl\n",
      "Indexing pipeline finished (Qdrant + Local BM25).\n",
      "Building BM25 index…\n",
      "Saved Local BM25 index to: C:\\Users\\KritChaJ\\OneDrive\\Documents\\Project CPE\\notebooks\\Database\\bm25_index.pkl\n",
      "Indexing pipeline finished (Qdrant + Local BM25).\n"
     ]
    }
   ],
   "source": [
    "# Indexing pipeline: Chunks → Embeddings (bge-m3) → Qdrant + Local BM25 (pickle)\n",
    "import os, json, time, hashlib, uuid, re, pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "# ติดตั้งไลบรารีที่จำเป็น (ตัด OpenSearch ออก ใช้ Local BM25 แทน)\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', *pkgs])\n",
    "    except Exception as e:\n",
    "        print('Package install finished with message:', e)\n",
    "\n",
    "pip_install(['sentence-transformers','qdrant-client','rank-bm25','tqdm'])\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------\n",
    "## คอนฟิกพื้นฐานจาก ENV\n",
    "QDRANT_URL = os.getenv('QDRANT_URL', 'http://127.0.0.1:6333')\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY', None)\n",
    "QDRANT_COLLECTION = os.getenv('QDRANT_COLLECTION', 'kb_chunks')\n",
    "\n",
    "# แหล่ง chunks\n",
    "DB_CHUNKS = Path('Database') / 'chunks.jsonl'\n",
    "ALT_DIR = Path('output') / 'chunks'\n",
    "\n",
    "\n",
    "def iter_chunks() -> List[Dict]:\n",
    "    docs = []\n",
    "    if DB_CHUNKS.exists():\n",
    "        with DB_CHUNKS.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    docs.append(json.loads(line))\n",
    "    elif ALT_DIR.exists():\n",
    "        for fp in sorted(ALT_DIR.glob('*.jsonl')):\n",
    "            with fp.open('r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        docs.append(json.loads(line))\n",
    "    else:\n",
    "        raise FileNotFoundError('ไม่พบ chunks: ควรมี Database/chunks.jsonl หรือ output/chunks/*.jsonl')\n",
    "    # เติม id ถ้าไม่มี\n",
    "    for d in docs:\n",
    "        if 'id' not in d or not d.get('id'):\n",
    "            basis = f\"{d.get('path','')}|{d.get('page_start','')}|{d.get('page_end','')}|{(d.get('text','')[:64]).strip()}\"\n",
    "            d['id'] = hashlib.sha1(basis.encode('utf-8','ignore')).hexdigest()[:24]\n",
    "    return docs\n",
    "\n",
    "\n",
    "docs = iter_chunks()\n",
    "print(f'Loaded chunks: {len(docs)}')\n",
    "\n",
    "# ----------------------\n",
    "## ฝั่งเวกเตอร์ → Qdrant\n",
    "model_name = 'BAAI/bge-m3'\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "texts = [d.get('text','') for d in docs]\n",
    "if texts:\n",
    "    print('Encoding embeddings…')\n",
    "    embeddings = embedder.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
    "    dim = len(embeddings[0])\n",
    "    print(f'Embedding dim = {dim}')\n",
    "else:\n",
    "    embeddings = []\n",
    "    dim = 0\n",
    "    print('No texts to embed')\n",
    "\n",
    "\n",
    "def setup_qdrant(url: str, api_key: str | None):\n",
    "    try:\n",
    "        client = QdrantClient(\n",
    "    url=\"https://9c2d3f64-67cb-49bf-a307-da136cc9b9f2.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.52N3m9ijtSQUG_4lnTk3hsP7Y9WuUV5TfRGwHP114pg\",\n",
    ")\n",
    "        try:\n",
    "            client.get_collection(QDRANT_COLLECTION)\n",
    "        except Exception:\n",
    "            client.recreate_collection(\n",
    "                collection_name=QDRANT_COLLECTION,\n",
    "                vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n",
    "            )\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print('Qdrant setup error:', e)\n",
    "        return None\n",
    "\n",
    "\n",
    "qdrant = setup_qdrant(QDRANT_URL, QDRANT_API_KEY)\n",
    "if qdrant is not None and len(embeddings) > 0:\n",
    "    print('Upserting to Qdrant…')\n",
    "    batch = []\n",
    "    for d, vec in tqdm(list(zip(docs, embeddings)), total=len(docs)):\n",
    "        payload = {\n",
    "            'text': d.get('text',''),\n",
    "            'source': d.get('source'),\n",
    "            'path': d.get('path'),\n",
    "            'page': d.get('page'),\n",
    "            'page_start': d.get('page_start'),\n",
    "            'page_end': d.get('page_end'),\n",
    "            'owner': d.get('owner'),\n",
    "            'sensitivity': d.get('sensitivity'),\n",
    "            'updated_at': d.get('updated_at'),\n",
    "            'tokens_est': d.get('tokens_est'),\n",
    "        }\n",
    "        qid = uuid.uuid5(uuid.NAMESPACE_URL, str(d['id']))\n",
    "        batch.append(PointStruct(id=str(qid), vector=vec.tolist(), payload=payload))\n",
    "        if len(batch) >= 128:\n",
    "            qdrant.upsert(collection_name=QDRANT_COLLECTION, points=batch)\n",
    "            batch = []\n",
    "    if batch:\n",
    "        qdrant.upsert(collection_name=QDRANT_COLLECTION, points=batch)\n",
    "    print('Qdrant upsert done.')\n",
    "else:\n",
    "    print('Skip Qdrant (no client or no embeddings)')\n",
    "\n",
    "# ----------------------\n",
    "## ฝั่งคีย์เวิร์ด → Local BM25 (pickle)\n",
    "try:\n",
    "    from pythainlp.tokenize import word_tokenize as th_word_tokenize\n",
    "    def tokenize(text: str) -> list[str]:\n",
    "        return [t.strip().lower() for t in th_word_tokenize(text or '', engine='newmm') if t.strip()]\n",
    "    tokenizer_name = 'thai-newmm'\n",
    "except Exception:\n",
    "    def tokenize(text: str) -> list[str]:\n",
    "        return [t.strip().lower() for t in (text or '').split() if t.strip()]\n",
    "    tokenizer_name = 'simple-space'\n",
    "\n",
    "print(f'Tokenizer = {tokenizer_name}')\n",
    "\n",
    "corpus_tokens = [tokenize(d.get('text','')) for d in docs]\n",
    "print('Building BM25 index…')\n",
    "bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "bm25_store = {\n",
    "    'type': 'BM25Okapi',\n",
    "    'tokenizer': tokenizer_name,\n",
    "    'created_at': time.time(),\n",
    "    'doc_ids': [d['id'] for d in docs],\n",
    "    'metadata': [\n",
    "        {\n",
    "            'id': d['id'],\n",
    "            'source': d.get('source'),\n",
    "            'path': d.get('path'),\n",
    "            'page': d.get('page'),\n",
    "            'page_start': d.get('page_start'),\n",
    "            'page_end': d.get('page_end'),\n",
    "            'owner': d.get('owner'),\n",
    "            'sensitivity': d.get('sensitivity'),\n",
    "            'updated_at': d.get('updated_at'),\n",
    "            'text': d.get('text',''),\n",
    "        } for d in docs\n",
    "    ],\n",
    "    'bm25': bm25,\n",
    "}\n",
    "\n",
    "bm25_path = Path('Database') / 'bm25_index.pkl'\n",
    "bm25_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with bm25_path.open('wb') as f:\n",
    "    pickle.dump(bm25_store, f)\n",
    "print(f'Saved Local BM25 index to: {bm25_path.resolve()}')\n",
    "\n",
    "\n",
    "# helper: โหลดและค้นหา BM25\n",
    "import numpy as np\n",
    "\n",
    "def load_bm25(path: Path | str = bm25_path):\n",
    "    p = Path(path)\n",
    "    with p.open('rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def bm25_search(query: str, top_k: int = 10, idx: dict | None = None) -> List[Dict]:\n",
    "    index = idx or load_bm25()\n",
    "    q_tokens = tokenize(query)\n",
    "    scores = index['bm25'].get_scores(q_tokens)\n",
    "    order = np.argsort(scores)[::-1][:top_k]\n",
    "    results = []\n",
    "    for i in order:\n",
    "        meta = index['metadata'][int(i)]\n",
    "        results.append({ **meta, 'score_bm25': float(scores[int(i)]) })\n",
    "    return results\n",
    "\n",
    "print('Indexing pipeline finished (Qdrant + Local BM25).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "868ccee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Retrieval: Local BM25 + Qdrant (RRF fusion)\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Filter, FieldCondition, MatchValue\n",
    "\n",
    "# ใช้ embedder/qdrant จากเซลล์ก่อนหน้า ถ้าไม่มีให้สร้างใหม่แบบ fallback\n",
    "try:\n",
    "    embedder\n",
    "except NameError:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    EMB_MODEL = os.getenv('EMB_MODEL', 'BAAI/bge-m3')\n",
    "    embedder = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "QDRANT_URL = os.getenv('QDRANT_URL', 'http://127.0.0.1:6333')\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY', None)\n",
    "QDRANT_COLLECTION = os.getenv('QDRANT_COLLECTION', 'kb_chunks')\n",
    "\n",
    "try:\n",
    "    qdr\n",
    "except NameError:\n",
    "    qdr = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY or None)\n",
    "\n",
    "RRF_K = int(os.getenv('RRF_K', 60))\n",
    "\n",
    "\n",
    "def _make_filter(owner: Optional[str], sensitivity: Optional[str]) -> Optional[Filter]:\n",
    "    must = []\n",
    "    if owner:\n",
    "        must.append(FieldCondition(key='owner', match=MatchValue(value=owner)))\n",
    "    if sensitivity:\n",
    "        must.append(FieldCondition(key='sensitivity', match=MatchValue(value=sensitivity)))\n",
    "    return Filter(must=must) if must else None\n",
    "\n",
    "\n",
    "def topk_vector(query: str, k: int = 30, owner: str | None = None, sensitivity: str | None = None) -> List[Dict]:\n",
    "    qvec = embedder.encode(query, normalize_embeddings=True).tolist()\n",
    "    flt = _make_filter(owner, sensitivity)\n",
    "    hits = qdr.search(\n",
    "        collection_name=QDRANT_COLLECTION,\n",
    "        query_vector=qvec,\n",
    "        limit=k,\n",
    "        query_filter=flt,\n",
    "        with_payload=True,\n",
    "        with_vectors=False,\n",
    "    )\n",
    "    out = []\n",
    "    for r, h in enumerate(hits, 1):\n",
    "        pl = h.payload or {}\n",
    "        out.append({\n",
    "            'id': h.id,\n",
    "            'text': pl.get('text', ''),\n",
    "            'meta': pl,\n",
    "            'vec_rank': r,\n",
    "            'vec_score': float(h.score),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "def topk_bm25_local(query: str, k: int = 30, owner: str | None = None, sensitivity: str | None = None) -> List[Dict]:\n",
    "    # ใช้ BM25 ที่บันทึกไว้ (และ helper bm25_search) จากเซลล์ก่อนหน้า\n",
    "    try:\n",
    "        idx = load_bm25()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError('BM25 index not found. Run the indexing cell first.') from e\n",
    "\n",
    "    # ดึงมามากกว่าที่ต้องการเล็กน้อยก่อน แล้วค่อยกรองตาม owner/sensitivity\n",
    "    initial = bm25_search(query, top_k=max(k * 5, 100), idx=idx)\n",
    "    out = []\n",
    "    rank = 0\n",
    "    for rec in initial:\n",
    "        meta = rec\n",
    "        if owner and meta.get('owner') != owner:\n",
    "            continue\n",
    "        if sensitivity and meta.get('sensitivity') != sensitivity:\n",
    "            continue\n",
    "        rank += 1\n",
    "        out.append({\n",
    "            'id': meta['id'],\n",
    "            'text': meta.get('text', ''),\n",
    "            'meta': meta,\n",
    "            'bm25_rank': rank,\n",
    "            'bm25_score': float(meta.get('score_bm25', 0.0)),\n",
    "        })\n",
    "        if len(out) >= k:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "def hybrid_rrf(query: str, k_vec: int = 30, k_bm25: int = 30, topk: int = 12,\n",
    "               owner: str | None = None, sensitivity: str | None = None,\n",
    "               rrf_k: int | None = None) -> List[Dict]:\n",
    "    rrf_c = rrf_k if rrf_k is not None else RRF_K\n",
    "    v = topk_vector(query, k_vec, owner, sensitivity)\n",
    "    b = topk_bm25_local(query, k_bm25, owner, sensitivity)\n",
    "\n",
    "    bank: Dict[str, Dict] = {}\n",
    "    rrf: Dict[str, float] = {}\n",
    "\n",
    "    for d in v:\n",
    "        k = str(d['id'])\n",
    "        bank.setdefault(k, d)\n",
    "        rrf[k] = rrf.get(k, 0.0) + 1.0 / (rrf_c + d['vec_rank'])\n",
    "    for d in b:\n",
    "        k = str(d['id'])\n",
    "        bank.setdefault(k, d)\n",
    "        rrf[k] = rrf.get(k, 0.0) + 1.0 / (rrf_c + d['bm25_rank'])\n",
    "\n",
    "    merged = []\n",
    "    for k, score in rrf.items():\n",
    "        base = bank[k]\n",
    "        merged.append({ **base, 'id': k, 'rrf': float(score) })\n",
    "    merged.sort(key=lambda x: x['rrf'], reverse=True)\n",
    "    return merged[:topk]\n",
    "\n",
    "# ตัวอย่างการใช้งาน (แก้ query แล้วรัน):\n",
    "# results = hybrid_rrf('หลักเกณฑ์การรับสมัคร', k_vec=30, k_bm25=30, topk=12)\n",
    "# for i, r in enumerate(results, 1):\n",
    "#     print(i, r.get('meta',{}).get('path'), r.get('rrf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76b45ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank (Cross-Encoder) for better final ordering\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "RERANK_MODEL = os.getenv('RERANK_MODEL', 'BAAI/bge-reranker-v2-m3')\n",
    "# หมายเหตุ: โมเดลนี้จะดาวน์โหลดครั้งแรก อาจใช้เวลาหลายนาที\n",
    "reranker = CrossEncoder(RERANK_MODEL)\n",
    "\n",
    "\n",
    "def rerank(query: str, docs: List[Dict], topk: int = 6) -> List[Dict]:\n",
    "    if not docs:\n",
    "        return []\n",
    "    pairs = [(query, d.get('text', '')) for d in docs]\n",
    "    scores = reranker.predict(pairs)\n",
    "    try:\n",
    "        scores = scores.tolist()\n",
    "    except Exception:\n",
    "        scores = list(scores)\n",
    "    for d, s in zip(docs, scores):\n",
    "        d['rerank'] = float(s)\n",
    "    return sorted(docs, key=lambda x: x.get('rerank', 0.0), reverse=True)[:topk]\n",
    "\n",
    "\n",
    "def hybrid_rrf_then_rerank(query: str,\n",
    "                           k_vec: int = 30,\n",
    "                           k_bm25: int = 30,\n",
    "                           topk_rrf: int = 20,\n",
    "                           topk_final: int = 8,\n",
    "                           owner: str | None = None,\n",
    "                           sensitivity: str | None = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    1) ดึงผลแบบ Hybrid (RRF) จาก Qdrant + Local BM25\n",
    "    2) ใช้ Cross-Encoder rerank เพื่อเรียงผลลัพธ์สุดท้าย\n",
    "    \"\"\"\n",
    "    base = hybrid_rrf(query, k_vec=k_vec, k_bm25=k_bm25, topk=topk_rrf,\n",
    "                      owner=owner, sensitivity=sensitivity)\n",
    "    return rerank(query, base, topk=topk_final)\n",
    "\n",
    "# ตัวอย่างการใช้งาน (แก้ query แล้วรัน):\n",
    "# rrf_results = hybrid_rrf('ระเบียบการรับสมัคร', k_vec=30, k_bm25=30, topk=20)\n",
    "# final_results = rerank('ระเบียบการรับสมัคร', rrf_results, topk=8)\n",
    "# หรือแบบ one-shot:\n",
    "# final_results = hybrid_rrf_then_rerank('ระเบียบการรับสมัคร', k_vec=30, k_bm25=30, topk_rrf=20, topk_final=8)\n",
    "# for i, r in enumerate(final_results, 1):\n",
    "#     print(i, r.get('meta', {}).get('path'), r.get('rerank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890fd470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551e67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c858534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3382cff3",
   "metadata": {},
   "source": [
    "## Context packing + citations (token-budget aware)\n",
    "\n",
    "- Packs hybrid (or reranked) results into a single context string respecting a token budget\n",
    "- Adds bracketed citations like `[1] source:page` and returns a map for rendering footnotes\n",
    "- Uses a Hugging Face tokenizer if available; falls back to a simple Thai-friendly estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f44d304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context packer using tokenizer: heuristic-4chars-per-token\n"
     ]
    }
   ],
   "source": [
    "# Token-budget context packer + citations + prompt builder\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Try to use a real tokenizer (Hugging Face). If unavailable, fall back to a simple estimator\n",
    "try:\n",
    "    from transformers import AutoTokenizer  # type: ignore\n",
    "    LLM_TOKENIZER = os.getenv('LLM_TOKENIZER', 'meta-llama/Llama-3-8b')\n",
    "    _tok = AutoTokenizer.from_pretrained(LLM_TOKENIZER, use_fast=True, trust_remote_code=True)\n",
    "    def _count_tokens(text: str) -> int:\n",
    "        return len(_tok.encode(text, add_special_tokens=False))\n",
    "    _TOKENIZER_NAME = LLM_TOKENIZER\n",
    "except Exception:\n",
    "    _tok = None\n",
    "    def _count_tokens(text: str) -> int:\n",
    "        # Thai heuristic: ~4 chars/token\n",
    "        t = max(1, int(round(len(text) / 4)))\n",
    "        return t\n",
    "    _TOKENIZER_NAME = 'heuristic-4chars-per-token'\n",
    "\n",
    "print('Context packer using tokenizer:', _TOKENIZER_NAME)\n",
    "\n",
    "\n",
    "def pack_context(chunks: List[Dict], budget_tokens: int = 1200) -> Tuple[str, Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Pack chunks into a single context string under a token budget.\n",
    "    Each chunk gets a bracketed index [i] and a citation string built from metadata.\n",
    "\n",
    "    Input chunk schema (expected fields):\n",
    "      - 'text': the chunk text\n",
    "      - 'meta': dict with fields like 'url' or ('source', 'page')\n",
    "\n",
    "    Returns:\n",
    "      - ctx: concatenated context string\n",
    "      - cites: mapping from i -> citation string for rendering\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return '', {}\n",
    "\n",
    "    packed = []\n",
    "    used = 0\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        meta = c.get('meta') or {}\n",
    "        cite = meta.get('url')\n",
    "        if not cite:\n",
    "            src = meta.get('source') or meta.get('path') or 'unknown'\n",
    "            pg = meta.get('page')\n",
    "            cite = f\"{src}:{pg}\" if pg is not None else str(src)\n",
    "        text = c.get('text', '') or ''\n",
    "        block = f\"[{i}] {cite}\\n{text}\\n\"\n",
    "        t = _count_tokens(block)\n",
    "        if used + t > budget_tokens:\n",
    "            break\n",
    "        packed.append((i, cite, text))\n",
    "        used += t\n",
    "\n",
    "    ctx = \"\\n\\n\".join([f\"[{i}] {text}\" for i, _, text in packed])\n",
    "    cites = {i: c for i, c, _ in packed}\n",
    "    return ctx, cites\n",
    "\n",
    "\n",
    "def build_prompt(question: str, ctx: str, cites: Dict[int, str]) -> str:\n",
    "    cite_list = \"\\n\".join([f\"[{i}] {c}\" for i, c in cites.items()])\n",
    "    return (\n",
    "        f\"คำถาม:\\n{question}\\n\\n\"\n",
    "        f\"บริบท:\\n{ctx}\\n\\n\"\n",
    "        f\"อ้างอิง:\\n{cite_list}\\n\"\n",
    "    )\n",
    "\n",
    "# Example usage (uncomment to try):\n",
    "# candidates = hybrid_rrf_then_rerank('ทุนการศึกษา', topk_rrf=20, topk_final=8)\n",
    "# ctx, cites = pack_context(candidates, budget_tokens=1200)\n",
    "# prompt = build_prompt('หลักเกณฑ์ทุนการศึกษาเป็นอย่างไร?', ctx, cites)\n",
    "# print(prompt[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a52e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de3f1386",
   "metadata": {},
   "source": [
    "## Language detect + Router → Pathumma / LLaMA\n",
    "\n",
    "- Detects Thai vs Latin ratio to choose a default model\n",
    "- Router rules: Thai-heavy or gov/legal keywords → Pathumma; otherwise LLaMA\n",
    "- OpenAI-compatible callers for both backends (set via ENV)\n",
    "- Orchestrator: retrieve → rerank → pack → route → generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a07e88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language detect + Router stub → Pathumma / LLaMA (OpenAI-compatible)\n",
    "import os, re, requests\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "# Reuse script_ratios if earlier cell defined it, else define fallback\n",
    "try:\n",
    "    script_ratios\n",
    "except NameError:\n",
    "    _TH_RANGE = re.compile(r'[\\u0E00-\\u0E7F]')\n",
    "    _LATIN_RANGE = re.compile(r'[A-Za-z]')\n",
    "    def script_ratios(text: str) -> tuple[float, float]:\n",
    "        if not text: return 0.0, 0.0\n",
    "        th = len(_TH_RANGE.findall(text)); la = len(_LATIN_RANGE.findall(text))\n",
    "        tot = th + la\n",
    "        return (th / tot if tot else 0.0, la / tot if tot else 0.0)\n",
    "\n",
    "\n",
    "def detect_language(text: str) -> Dict[str, float | str]:\n",
    "    th_r, la_r = script_ratios(text or '')\n",
    "    if th_r >= 0.8: label = 'th'\n",
    "    elif la_r >= 0.8: label = 'en'\n",
    "    elif th_r >= 0.4 and la_r >= 0.2: label = 'mixed'\n",
    "    else: label = 'unknown'\n",
    "    return {'thai_ratio': th_r, 'latin_ratio': la_r, 'label': label}\n",
    "\n",
    "# Keywords to force Pathumma\n",
    "_TH_KEYWORDS = [r'ระเบียบ', r'ประกาศ', r'ข้อบังคับ', r'กฎหมาย', r'ราชการ', r'ทะเบียน', r'ทุนการศึกษา']\n",
    "\n",
    "def route_model(question: str, ctx: str = '', hint: str | None = None) -> str:\n",
    "    if hint in ('pathumma','llama'):  # manual override\n",
    "        return hint\n",
    "    th_q, _ = script_ratios(question or '')\n",
    "    th_c, _ = script_ratios(ctx or '')\n",
    "    if th_q + th_c >= 0.55:\n",
    "        return 'pathumma'\n",
    "    for kw in _TH_KEYWORDS:\n",
    "        if re.search(kw, question or '') or re.search(kw, ctx or ''):\n",
    "            return 'pathumma'\n",
    "    return 'llama'\n",
    "\n",
    "# OpenAI-compatible caller\n",
    "\n",
    "def _openai_chat(base_url: str, api_key: str, model: str, messages: List[Dict], temperature: float = 0.2, max_tokens: int = 512) -> str:\n",
    "    url = f\"{base_url.rstrip('/')}/chat/completions\"\n",
    "    headers = {'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'}\n",
    "    payload = {'model': model, 'messages': messages, 'temperature': float(temperature), 'max_tokens': int(max_tokens)}\n",
    "    r = requests.post(url, headers=headers, json=payload, timeout=120)\n",
    "    try:\n",
    "        r.raise_for_status()\n",
    "    except Exception:\n",
    "        print('GEN API ERROR:', r.status_code, r.text[:200])\n",
    "        raise\n",
    "    data = r.json()\n",
    "    return (data.get('choices') or [{}])[0].get('message', {}).get('content', '').strip()\n",
    "\n",
    "# Endpoints from ENV\n",
    "PATHUMMA_OPENAI_BASE = os.getenv('PATHUMMA_OPENAI_BASE', 'http://localhost:8080/v1')\n",
    "PATHUMMA_API_KEY     = os.getenv('PATHUMMA_API_KEY', 'placeholder')\n",
    "PATHUMMA_MODEL       = os.getenv('PATHUMMA_MODEL', 'pathumma-text-7b')\n",
    "\n",
    "LLAMA_OPENAI_BASE    = os.getenv('LLAMA_OPENAI_BASE', 'http://localhost:8000/v1')\n",
    "LLAMA_API_KEY        = os.getenv('LLAMA_API_KEY', 'placeholder')\n",
    "LLAMA_MODEL          = os.getenv('LLAMA_MODEL', 'llama-3.1-8b-instruct')\n",
    "\n",
    "SYSTEM_THAI = (\n",
    "    'คุณเป็นผู้ช่วยที่ยึดตามหลักฐานจากบริบท ตอบสั้น กระชับ และแสดงอ้างอิง [i]; '\n",
    "    'ถ้าไม่พบคำตอบให้บอกอย่างตรงไปตรงมา'\n",
    ")\n",
    "\n",
    "# Basic allow/deny (extend later)\n",
    "_DENY = [r'hack', r'แฮ็ก', r'ทำร้าย', r'phishing']\n",
    "\n",
    "def is_denied(q: str) -> bool:\n",
    "    ql = (q or '').lower()\n",
    "    return any(re.search(p, ql) for p in _DENY)\n",
    "\n",
    "# PII redaction (simple)\n",
    "_EMAIL = re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}')\n",
    "_PHONE = re.compile(r'(?<!\\d)(?:\\+?\\d{1,3}[- .]?)?(?:\\d{2,4}[- .]?){2,4}\\d{2,4}(?!\\d)')\n",
    "_NATID = re.compile(r'\\b\\d{13}\\b')\n",
    "\n",
    "def redact_pii(text: str) -> str:\n",
    "    t = _EMAIL.sub('[REDACTED_EMAIL]', text or '')\n",
    "    t = _PHONE.sub('[REDACTED_PHONE]', t)\n",
    "    t = _NATID.sub('[REDACTED_ID]', t)\n",
    "    return t\n",
    "\n",
    "\n",
    "def generate_answer_with_router(question: str, ctx: str, cites: Dict[int, str], hint: str | None = None,\n",
    "                                temperature: float = 0.2, max_tokens: int = 512) -> tuple[str, str]:\n",
    "    if is_denied(question):\n",
    "        return 'deny', 'คำขอไม่อยู่ในขอบเขตการให้บริการ'\n",
    "    model = route_model(question, ctx, hint=hint)\n",
    "    cite_list = '\\n'.join([f\"[{i}] {c}\" for i, c in cites.items()])\n",
    "    user_prompt = f\"คำถาม:\\n{question}\\n\\nบริบท:\\n{ctx}\\n\\nอ้างอิง:\\n{cite_list}\\n\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': SYSTEM_THAI},\n",
    "        {'role': 'user', 'content': user_prompt},\n",
    "    ]\n",
    "    if model == 'pathumma':\n",
    "        ans = _openai_chat(PATHUMMA_OPENAI_BASE, PATHUMMA_API_KEY, PATHUMMA_MODEL, messages, temperature, max_tokens)\n",
    "    else:\n",
    "        ans = _openai_chat(LLAMA_OPENAI_BASE, LLAMA_API_KEY, LLAMA_MODEL, messages, temperature, max_tokens)\n",
    "    ans = redact_pii(ans)\n",
    "    return model, ans\n",
    "\n",
    "\n",
    "def qa_with_router(question: str, k_vec=30, k_bm25=30, topk_rrf=20, topk_final=8, budget_tokens=1200,\n",
    "                   owner: str | None = None, sensitivity: str | None = None, hint: str | None = None) -> Dict:\n",
    "    # Retrieve + rerank\n",
    "    cands = hybrid_rrf_then_rerank(question, k_vec=k_vec, k_bm25=k_bm25, topk_rrf=topk_rrf, topk_final=topk_final,\n",
    "                                   owner=owner, sensitivity=sensitivity)\n",
    "    # Pack context\n",
    "    ctx, cites = pack_context(cands, budget_tokens=budget_tokens)\n",
    "    # Generate\n",
    "    model, answer = generate_answer_with_router(question, ctx, cites, hint=hint)\n",
    "    return {'model': model, 'answer': answer, 'cites': cites, 'used_chunks': cands, 'ctx_tokens_est': len(ctx)}\n",
    "\n",
    "# Example (uncomment to test after retrieval/index done):\n",
    "# out = qa_with_router('กำหนดการรับสมัครนักศึกษาใหม่')\n",
    "# print('MODEL:', out['model'])\n",
    "# print('ANSWER:', out['answer'][:500])\n",
    "# print('CITES:', out['cites'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90288f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a02e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "823268df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: กำหนดการรับสมัครนักศึกษาใหม่คืออะไร\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: กำหนดการรับสมัครนักศึกษาใหม่คืออะไร\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KritChaJ\\AppData\\Local\\Temp\\ipykernel_24664\\1416741584.py:40: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = qdr.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: กำหนดการรับสมัครนักศึกษาใหม่คืออะไร\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KritChaJ\\AppData\\Local\\Temp\\ipykernel_24664\\1416741584.py:40: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = qdr.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipeline error: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002112AD507D0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n"
     ]
    }
   ],
   "source": [
    "# ลองถาม RAG ว่าตอบได้ไหม\n",
    "def rag_try(question: str, hint: str | None = None):\n",
    "    print(f'QUESTION: {question}')\n",
    "    try:\n",
    "        out = qa_with_router(question, hint=hint)\n",
    "    except Exception as e:\n",
    "        print('RAG pipeline error:', e)\n",
    "        return {'error': str(e)}\n",
    "    used = out.get('used_chunks', [])\n",
    "    if not used:\n",
    "        print('No chunks retrieved.')\n",
    "    else:\n",
    "        print(f'Retrieved {len(used)} chunk(s); model={out.get(\"model\")}')\n",
    "    # แสดง citations\n",
    "    for i, cite in out.get('cites', {}).items():\n",
    "        print(f'[{i}] {cite}')\n",
    "    # แสดงบางส่วนของบริบท (ตัดให้สั้น)\n",
    "    ctx_preview = out.get('answer','')[:500]\n",
    "    print('\\nANSWER (preview ≤500 chars):\\n', ctx_preview)\n",
    "    return out\n",
    "\n",
    "# ทดสอบ 1 คำถาม (ปรับได้)\n",
    "_ = rag_try('กำหนดการรับสมัครนักศึกษาใหม่คืออะไร')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1163f5c",
   "metadata": {},
   "source": [
    "# Typhoon OCR Integration Overview\n",
    "#\n",
    "# This section demonstrates how to use `typhoon_ocr` to extract\n",
    "# layout-aware Markdown from PDF pages and images.\n",
    "#\n",
    "# Key points:\n",
    "# - `ocr_document(pdf_or_image_path, page_num=<int optional>)`\n",
    "#   When `page_num` is omitted for PDFs, library may default to first page;\n",
    "#   supply a page number to target a specific page.\n",
    "# - For multi-page PDFs, iterate page numbers and accumulate Markdown.\n",
    "# - Output is Markdown (headings, tables, lists if detected).\n",
    "# - Ensure the file paths exist; adjust placeholders below.\n",
    "# - Large PDFs: consider limiting pages or batching to avoid high runtime.\n",
    "# - Typical dependencies (already installed here): typhoon-ocr, pypdf.\n",
    "# - Save concatenated Markdown for downstream ingestion / vectorization.\n",
    "#\n",
    "# Adjust `pdf_path` and `image_path` to real files in your workspace.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
